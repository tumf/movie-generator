# Project Context

## Purpose

Movie Generator - A Python CLI tool that generates YouTube slide videos from blog URLs.

Key features:
- Automatic YouTube script generation from blog articles
- Multi-language support (Japanese, English, etc.)
- Multi-speaker dialogue with character animation
- Narration via VOICEVOX audio synthesis with pronunciation dictionary
- Automatic furigana generation for technical terms
- AI-powered slide generation with logo integration
- Character animation (static positioning, lip sync, blinking, sway/bounce)
- Phrase-based accurate subtitle synchronization with color coding
- Smooth slide transitions (fade, slide, wipe, flip, clockWipe)
- MCP integration for enhanced web scraping
- Unified styling via YAML configuration

## Tech Stack

- **Language**: Python 3.13
- **Package Manager**: uv
- **Audio**: VOICEVOX Core
- **Video**: Remotion (TypeScript/React)
- **LLM**: OpenRouter (GPT, Claude, Gemini, etc.)
- **Image Generation**: OpenRouter (image-capable models)
- **CLI**: Click + Rich
- **Parsing**: BeautifulSoup4, httpx
- **Morphological Analysis**: fugashi + unidic (for furigana)
- **Asset Management**: Pillow, PSD Tools, SVG processing
- **MCP**: Model Context Protocol (Firecrawl, etc.)

## Project Conventions

### Code Style

- Ruff for formatting/linting (line-length: 100)
- Type hints required (mypy strict mode)
- Docstrings in Google style
- Pydantic models for all data structures

### Architecture Patterns

- Modular pipeline design (content → script → audio → slides → video)
- Pydantic for configuration validation and serialization
- Dependency injection for testability
- Clear separation of concerns (content/, script/, audio/, slides/, video/, assets/, mcp/)

### Testing Strategy

- pytest for unit/integration tests
- Mocked LLM calls for reproducibility
- E2E tests with sample content
- Integration tests with actual LLM calls (marked with `@pytest.mark.integration`)
- Scene range filtering tests (--scenes N, N-M, N-)

### Git Workflow

- Conventional commits (feat, fix, docs, etc.)
- Feature branches: `feature/<change-id>`
- Pre-commit hooks for automated quality checks

## Domain Context

### Video Generation Pipeline

```
URL → Content Fetch (MCP/httpx) → Script Generation (LLM) →
Phrase Splitting → Audio Synthesis (VOICEVOX) →
Slide Generation (LLM + Image AI) → Video Rendering (Remotion)
```

### Key Concepts

- **Phrase**: Narration segment of 3-6 seconds with timing and section info
  - `section_index`: Which script section the phrase belongs to
  - `original_index`: Global phrase ID for file naming
- **Section**: Script section with title, narration(s), and slide prompt
- **Narration**: Text with `reading` field (katakana pronunciation) for VOICEVOX
- **Persona**: Speaker configuration (voice, character, subtitle color, animation)
- **Composition**: JSON data linking phrases, audio files, and slides for Remotion
- **Logo Asset**: Product/company logo downloaded from LLM-identified URLs
- **Character Animation**: Static positioning + lip sync + blinking + motion (sway/bounce)
- **Transition**: Smooth animations between slides (fade, slide, wipe, flip, clockWipe)

### Data Models

#### Core Script Structure
```python
class Narration(BaseModel):
    text: str              # Display text
    reading: str           # Katakana pronunciation (auto-generated by LLM)
    persona_id: str | None # Speaker ID (for multi-speaker)

class Section(BaseModel):
    title: str
    narrations: list[Narration]
    slide_prompt: str

class VideoScript(BaseModel):
    title: str
    sections: list[Section]
```

#### Configuration Models
```python
class PersonaConfig(BaseModel):
    id: str
    name: str
    character: str
    synthesizer: SynthesizerSettings
    subtitle_color: str = SubtitleConstants.DEFAULT_COLOR
    character_image: str | None
    character_position: Literal["left", "right", "center"]
    mouth_open_image: str | None
    eye_close_image: str | None
    animation_style: Literal["bounce", "sway", "static"]
```

## Important Constraints

- VOICEVOX environment dependent (Open JTalk dictionary, voice models)
- VOICEVOX Core requires manual installation (not available via PyPI)
- macOS/Linux prioritized (Windows support is best effort)
- Multi-language support (Japanese, English, etc.) via `language` config
- OpenRouter API key required for script/slide generation

## External Dependencies

- **VOICEVOX Core**: https://voicevox.hiroshiba.jp/
  - Manual installation required
  - Official documentation: https://voicevox.github.io/voicevox_core/apis/
  - Download from: https://github.com/VOICEVOX/voicevox_core/releases
- **OpenRouter API**: https://openrouter.ai/
  - Used for script generation and slide image generation
- **Remotion**: https://www.remotion.dev/
  - Node.js-based video rendering framework
- **MCP Servers** (Optional):
  - Firecrawl: Enhanced web scraping
  - Configuration: `opencode.jsonc`, `.cursor/mcp.json`, etc.

## Critical Implementation Notes

### LLM Prompt Management

**Golden Rule**: When adding features that require LLM output changes, ALL prompt variants MUST be updated with complete instructions.

**Prompt Variants**:
- `SCRIPT_GENERATION_PROMPT_JA` (single-speaker, Japanese)
- `SCRIPT_GENERATION_PROMPT_EN` (single-speaker, English)
- `SCRIPT_GENERATION_PROMPT_DIALOGUE_JA` (multi-speaker, Japanese)
- `SCRIPT_GENERATION_PROMPT_DIALOGUE_EN` (multi-speaker, English)

**Verification Checklist**:
- [ ] Field is in output format example
- [ ] Detailed generation instructions provided
- [ ] Edge cases and rules documented
- [ ] Examples demonstrate correct format
- [ ] Required vs optional is explicit
- [ ] Test with actual LLM calls (not just mocks)

### Pydantic Serialization

**ALWAYS use `model_dump()` to serialize Pydantic models**. Never manually construct dictionaries.

```python
# Good
narr_dict = n.model_dump(exclude_none=True, by_alias=True)

# Bad - manual serialization loses fields!
narr_dict = {"text": n.text, "reading": n.reading}
```

### Subtitle Color Management

**Single Source of Truth**: `constants.py` → `SubtitleConstants.DEFAULT_COLOR`

**Usage**:
- `config.py`: Default for `PersonaConfig.subtitle_color`
- `remotion_renderer.py`: Fallback in `_get_persona_fields()`
- `templates.py`: Embedded in generated VideoGenerator.tsx

**Never hardcode colors** - use constants to prevent regressions.

### Scene Range Filtering

**Always regenerate `composition.json`** when using `--scenes` filter to ensure accurate phrase-to-slide mapping.

**Test Coverage**:
- Single scene: `--scenes 2`
- Range: `--scenes 1-3`
- Open-ended: `--scenes 2-`

## Testing Quick Reference

```bash
# Run all tests
uv run pytest

# Run specific test file
uv run pytest tests/test_scene_range.py -v

# Run specific test function
uv run pytest tests/test_scene_range.py::TestSceneRangeParsing::test_single_scene -v

# Run integration tests (requires LLM API keys)
uv run pytest -m integration

# Run with coverage
uv run pytest --cov=src/movie_generator --cov-report=html
```

## CLI Quick Reference

```bash
# Generate from URL
uv run movie-generator generate https://example.com/blog-post

# Generate from script.yaml
uv run movie-generator generate path/to/script.yaml

# With custom config
uv run movie-generator generate https://example.com -c config.yaml

# Scene filtering
uv run movie-generator generate script.yaml --scenes 1-3

# MCP integration
uv run movie-generator generate https://example.com --mcp-config opencode.jsonc

# Placeholder mode (testing without VOICEVOX)
uv run movie-generator generate https://example.com --allow-placeholder

# Initialize config file
uv run movie-generator config init --output config.yaml
```
